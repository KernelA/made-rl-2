stages:
  precompute-minmax-tree:
    vars:
      - ./configs/minmax_precompute.yaml:out_dir
    cmd: python ./precompute_tree.py
    deps:
      - ./precompute_tree.py
      - ./configs/minmax_precompute.yaml
    outs:
      - ${out_dir}

  precompute-mcts-cross-4x4:
    vars:
      - out_dir: ./trees/mcts/4/cross
    cmd: python ./precompute_mctc_tree.py out_dir=${out_dir} cross_policy.tree.env='\${env}'
    deps:
      - ./precompute_mctc_tree.py
      - ./configs/mcts_precompute.yaml
    params:
      - ./configs/mcts_precompute.yaml:
          - num_simulation
    outs:
      - ${out_dir}

  precompute-mcts-cross-random:
    vars:
      - config_file: mcts_precompute_cross_3x3_random
      - ./configs/mcts_precompute_cross_3x3_random.yaml:out_dir
      - tree_file: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./precompute_mctc_tree.py -cn ${config_file}
    deps:
      - ./precompute_mctc_tree.py
      - ./configs/${config_file}.yaml
      - ${tree_file}
    params:
      - ./configs/${config_file}.yaml:
          - num_simulation
    outs:
      - ${out_dir}

  precompute-mcts-circle-random:
      vars:
        - config_file: mcts_precompute_circle_3x3_random
        - ./configs/mcts_precompute_circle_3x3_random.yaml:out_dir
        - tree_file: ./trees/minmax/3/cross/3_3_3_start_1.pickle
      cmd: python ./precompute_mctc_tree.py -cn ${config_file}
      deps:
        - ./precompute_mctc_tree.py
        - ./configs/${config_file}.yaml
        - ${tree_file}
      params:
        - ./configs/${config_file}.yaml:
            - num_simulation
      outs:
        - ${out_dir}
  

  precompute-mcts-circle:
    vars:
      - out_dir: ./trees/mcts/3/circle
      - tree_file: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./precompute_mctc_tree.py env=env3x3 num_simulation=10000 circle_policy=mcts_tree circle_policy.tree.depth_limit=null out_dir=${out_dir} cross_policy=tree_from_dump cross_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_file}
    deps:
      - ./precompute_mctc_tree.py
      - ./configs/mcts_precompute.yaml
    params:
      - ./configs/mcts_precompute.yaml:
          - num_simulation
    outs:
      - ${out_dir}

  precompute-mcts-circle-4x4:
    vars:
      - out_dir: ./trees/mcts/4/circle
    cmd: python ./precompute_mctc_tree.py out_dir=${out_dir} cross_policy=random circle_policy=mcts_tree circle_policy.tree.env=\${env}
    deps:
      - ./precompute_mctc_tree.py
      - ./configs/mcts_precompute.yaml
    params:
      - ./configs/mcts_precompute.yaml:
          - num_simulation
    outs:
      - ${out_dir}

  precompute-states-3x3-cross:
    vars:
      - out_path: ./stat/3/cross_stat.json
    cmd: python ./generate_all_states.py start_player=1 state_file=${out_path}
    deps:
      - ./generate_all_states.py
      - ./configs/generate_states.yaml
      - ./trees/minmax/3/cross/3_3_3_start_1.pickle
    outs:
      - ${out_path}

  precompute-states-3x3-circle:
    vars:
      - out_path: ./stat/3/circle_stat.json
    cmd: python ./generate_all_states.py start_player=-1 state_file=${out_path}
    deps:
      - ./generate_all_states.py
      - ./configs/generate_states.yaml
      - ./trees/minmax/3/cross/3_3_3_start_1.pickle
    outs:
      - ${out_path}

  compute-minmax-game-cross-minmax:
    vars:
      - ./configs/simulate.yaml
      - out_dir: stat/cross_minmax_vs_random/
      - tree_file: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./simulate_game.py -m cross_policy=tree_from_dump cross_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_file} circle_policy=random random_action_proba=0,0.1,0.2,0.5,0.7,0.9 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/simulate.yaml:
          - num_sim
    deps:
      - ./simulate_game.py
      - ./configs/simulate.yaml
      - ${tree_file}
    outs:
      - ${out_dir}

  compute-minmax-game-circle-minmax:
    vars:
      - ./configs/simulate.yaml
      - out_dir: stat/cross_random_vs_minmax/
      - tree_file: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./simulate_game.py -m cross_policy=random circle_policy=tree_from_dump circle_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_file} random_action_proba=0,0.1,0.2,0.5,0.7,0.9 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/simulate.yaml:
          - num_sim
    deps:
      - ./simulate_game.py
      - ./configs/simulate.yaml
      - ${tree_file}
    outs:
      - ${out_dir}

  q-table-learning-train-cross:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/table_q_learning/cross
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m circle_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=0.5,0.8,0.9 q_learner.gamma=0.1,0.5,0.8 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}

  q-table-learning-train-circle:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/table_q_learning/circle
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m circle_policy=q_learning cross_policy=tree_from_dump cross_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=0.5,0.8,0.9 q_learner.gamma=0.1,0.5,0.8 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}

  q-table-learning-train-cross-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/table_q_learning/4/cross
    cmd: python ./q_learn_params.py -m env=env4x4 num_train_iterations=900000 cross_policy=q_learning circle_policy=random q_learner.alpha=0.5,0.8,0.9 q_learner.gamma=0.1,0.5,0.8 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}

  q-table-learning-train-circle-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/table_q_learning/4/circle
    cmd: python ./q_learn_params.py -m env=env4x4 num_train_iterations=900000 circle_policy=q_learning cross_policy=random q_learner.alpha=0.5,0.8,0.9 q_learner.gamma=0.1,0.5,0.8 hydra.sweep.dir=${out_dir}
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}

  dqn-learning-train-cross:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/dqn_learning/cross
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m cross_policy=network_policy circle_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.QNeuralNetworkSimulation cross_policy.model.n_rows='\${env.n_rows}' cross_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=30000
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}

  dqn-learning-train-circle:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/dqn_learning/circle
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m cross_policy=tree_from_dump circle_policy=network_policy cross_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.QNeuralNetworkSimulation circle_policy.model.n_rows='\${env.n_rows}' circle_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=30000
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}
  
  dqn-learning-train-cross-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/dqn_learning/4/cross
    cmd: python ./q_learn_params.py -m env=env4x4 cross_policy=network_policy circle_policy=random q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.QNeuralNetworkSimulation cross_policy.model.n_rows='\${env.n_rows}' cross_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=50000
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}

  dqn-learning-train-circle-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/dqn_learning/4/circle
    cmd: python ./q_learn_params.py -m env=env4x4 cross_policy=random circle_policy=network_policy q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.QNeuralNetworkSimulation circle_policy.model.n_rows='\${env.n_rows}' circle_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=50000
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}

  duoling-dqn-learning-train-circle:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/duoling_dqn_learning/circle
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m cross_policy=tree_from_dump circle_policy=duoling_network cross_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.DuelingQNeuralNetworkSimulation circle_policy.model.n_rows='\${env.n_rows}' circle_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=30000 +q_learner.target_update=10
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}

  duoling-dqn-learning-train-cross:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/duoling_dqn_learning/cross
      - tree_path: ./trees/minmax/3/cross/3_3_3_start_1.pickle
    cmd: python ./q_learn_params.py -m circle_policy=tree_from_dump cross_policy=duoling_network circle_policy.tree.path_to_file='\${hydra:runtime.cwd}'/${tree_path} q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.DuelingQNeuralNetworkSimulation cross_policy.model.n_rows='\${env.n_rows}' cross_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=30000 +q_learner.target_update=10
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
      - ${tree_path}
    outs:
      - ${out_dir}

  duoling-dqn-learning-train-circle-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/duoling_dqn_learning/4/circle
    cmd: python ./q_learn_params.py -m cross_policy=random circle_policy=duoling_network q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.DuelingQNeuralNetworkSimulation circle_policy.model.n_rows='\${env.n_rows}' circle_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=70000 +q_learner.target_update=10
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}

  duoling-dqn-learning-train-cross-4x4:
    vars:
      - ./configs/table_q_learning.yaml
      - out_dir: ./exp/duoling_dqn_learning/4/cross
    cmd: python ./q_learn_params.py -m circle_policy=random cross_policy=duoling_network q_learner.alpha=1e-4,1e-5,1e-6 optimizer.lr='\${q_learner.alpha}' q_learner.gamma=0.1,0.5,0.25 hydra.sweep.dir=${out_dir} q_learner._target_=tictac_rl.q_learning.q_learning.DuelingQNeuralNetworkSimulation cross_policy.model.n_rows='\${env.n_rows}' cross_policy.model.n_cols='\${env.n_cols}' +q_learner.memory_capacity=10000 +q_learner.batch_size=128 +q_learner.device=cpu num_train_iterations=50000 +q_learner.target_update=10
    params:
      - ./configs/table_q_learning.yaml:
          - num_train_iterations
          - num_test_iterations
          - num_valid_episodes
          - tree_random_action_proba
    deps:
      - ./q_learn_params.py
      - ./configs/table_q_learning.yaml
    outs:
      - ${out_dir}